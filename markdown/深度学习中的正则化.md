---
title:  深度学习中的正则化
date: 2017-12-27
tags: [正则化,深度学习,过拟合]
toc: true
mathjax: true
categories: Maching Learning
---

本文整理总结自 《Deep Learning》花书
此处将正则化定义为  **对学习算法的修改——旨在减少泛化误差而不是训练误差**

1. 参数范数惩罚
    * L1 Lasso，L1正则化会产生更稀疏(sparse)的解，被广泛用于特征选择
    * L2 Ridge 岭回归，权重更加接近原点。在无助于目标函数减小的方向的分量会被正则化衰减掉
2. 作为约束的范数惩罚
    * 对于Ω(θ) < k，如果我们已经知道什么样的 k 是合适的，而不想花时间寻找对应于此 k 处的 α 值，这会非常有用
    * 惩罚可能会导致目标函数非凸而使算法陷入局部极小 (对应于小的 θ)
    * 重投影的显式约束还对优化过程增加了一定的稳定性
<!-- More -->
3. 数据集增强
    * 操纵图像以生产新的数据
    * 在神经网络的输入层注入噪音
4. 噪声鲁棒性
    * 向网络权重添加随机扰动。这种形式的正则化鼓励参数进入权重小扰动对输出相对影响较小的参数空间区域。换句话说，它推动模型进入对权重小的变化相对不敏感的区域，找到的点不只是极小点，还是由平坦区域所包围的最小点。
    * 向输出目标注入噪声。设，对于一些小常数 ε，训练集标记 y 是正确的概率是 1 − ε，(以 ε 的概率)任何其他可能的标签也可能是正确的。标签平滑(label smoothing)通过把确切分类目标从 0 和1 替换成 ε ／（k-1）和 1 − ε，正则化具有 k 个输出的 softmax 函数 的模型。标准交叉熵k−1损失可以用在这些非确切目标的输出上。使用 softmax 函数 和明确目标的最大似然学习可能永远不会收敛——softmax 函数 永远无法真正预测 0 概率或 1 概率，因此它会继续学习越来越大的权重，使预测更极端。使用如权重衰减等其他正则化策略能够防止这种情况。标签平滑的优势是能够防止模型追求确切概率而不影响模型学习正确分类。
5. 半监督学习
    * 无监督学习可以为如何在表示空间聚集样本提供有用线索。生成模型 P ( x) 或 P (x , y) 与判别模型 P (y |x  )共享参数，而不用分离无监督和监督部分。大大提升了 P (y |x  )的效果
6. 多任务学习
    * 通过合并几个任务中的样例(可以视为对参数施加的软约束)来提高泛化的一种方式。额外的训练样本以同样的方式将模型的参数推向泛化更好的方向，当模型的一部分在任务之间共享时，模型的这一部分更多地被约束为良好的值(假设共享是合理的)，往往能更好地泛化
    * ![sharing_var.png](https://i.loli.net/2017/12/28/5a43d6abf1e74.png)
    * 如图中所示，由x预测y(1), y(2), y(3)时，不分别训练三个独立模型，而是三个模型共享一部分隐藏单元（前提是这几个输出任务是有一定关联的，也即存在存在解释输入   变化的共同因素池）一些顶层因素不与输出任务 (h (3)) 的任意一个关联是有意义的:这些因素可以解释一些输入变化但与预测 y(1) 或 y(2) 不相关
7. 提前终止
    * 当我们的模型表示能力足够强时，训练误差会随着时间的推移逐渐下降但验证集的误差在降到最低点时会再次上升。
    * 当验证集上的误差在一定的循环次数内没有获得改善时，算法会终止
    * 为什么提前终止具有正则化效果？假设梯度有界，限制迭代的次数和学习速率能够限制从θ0 到达的参数空间的大小。
8. 参数绑定和参数共享
    * 将各种模型或模型组件解释为共享唯一的一组参数
    * 典型：卷积神经网络
9. bagging 等其他集成方法
    * Bagging是通过结合几个模型降低泛化误差的技术。主要想法是分别训练几个不同的模型，然后让所有模型表决测试样例的输出。这是机器学习中常规策略的一个例子，被称为模型平均(modelaveraging)。采用这种策略的技术被称为集成方法。
    * 模型平均(model averaging)奏效的原因是不同的模型通常不会在测试集上产生完全相同的误差。成员的误差越独立的，集成将显著更加比其成员表现得好。如果n个模型间的误差完全独立且均为p，集合模型的误差降降至p／n。
    * 这也是Kaggle等数据竞赛的必备法宝之一
10. Dropout
  1. 在一种近似下，Dropout可以被认为是集成大量深层神经网络的实用Bagging方法
  2. 计算方便但功能强大。不怎么限制适用的模型或训练过程。几乎在所有使用分布式表示且可以用随机梯度下降训练的模型上都表现很好。
  3. 另一个观点：Dropout正则化每个隐藏单元不仅是一个很好的特征，更要在许多情况下是良好的特征。
11. GAN
    * 由训练好的模型提供标签产生的对抗样本被（虚拟对抗样本）实现半监督学习的方法。神经网络能够将函数从接近线性转化为局部近似恒定，从而可以灵活地捕获到训练数据中的线性趋势同时学习抵抗局部扰动。
    * 对抗训练通过鼓励网络在训练数据附近的局部区域恒定来限制这一高度敏感的局部线性行为。这可以被看作是一种明确地向监督神经网络引入局部恒定先验的方法。